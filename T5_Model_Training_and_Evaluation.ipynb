{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# H∆∞·ªõng d·∫´n Train v√† ƒê√°nh gi√° M√¥ h√¨nh venify/googleT5\n",
        "\n",
        "## M·ª•c l·ª•c\n",
        "1. [Gi·ªõi thi·ªáu v·ªÅ m√¥ h√¨nh T5](#gioi-thieu)\n",
        "2. [C√°c metrics ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng](#metrics)\n",
        "3. [Chu·∫©n b·ªã d·ªØ li·ªáu](#du-lieu)\n",
        "4. [Training m√¥ h√¨nh](#training)\n",
        "5. [ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng m√¥ h√¨nh](#danh-gia)\n",
        "6. [So s√°nh k·∫øt qu·∫£](#so-sanh)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Gi·ªõi thi·ªáu v·ªÅ m√¥ h√¨nh T5 {#gioi-thieu}\n",
        "\n",
        "**T5 (Text-to-Text Transfer Transformer)** l√† m·ªôt m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi Google. M√¥ h√¨nh n√†y:\n",
        "- Chuy·ªÉn ƒë·ªïi m·ªçi t√°c v·ª• NLP th√†nh ƒë·ªãnh d·∫°ng text-to-text\n",
        "- S·ª≠ d·ª•ng ki·∫øn tr√∫c Transformer encoder-decoder\n",
        "- R·∫•t hi·ªáu qu·∫£ cho c√°c t√°c v·ª• nh∆∞ d·ªãch thu·∫≠t, t√≥m t·∫Øt vƒÉn b·∫£n, sinh c√¢u h·ªèi\n",
        "\n",
        "**venify/googleT5** l√† m·ªôt bi·∫øn th·ªÉ ƒë∆∞·ª£c fine-tune ƒë·∫∑c bi·ªát cho c√°c t√°c v·ª• gi√°o d·ª•c ti·∫øng Anh.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. C√°c Metrics ƒê√°nh Gi√° Ch·∫•t L∆∞·ª£ng {#metrics}\n",
        "\n",
        "### 2.1 BLEU Score\n",
        "- **ƒê·ªãnh nghƒ©a**: Bilingual Evaluation Understudy\n",
        "- **M·ª•c ƒë√≠ch**: ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c (precision) c·ªßa vƒÉn b·∫£n sinh ra\n",
        "- **Ph·∫°m vi**: 0-1 (c√†ng cao c√†ng t·ªët)\n",
        "- **∆Øu ƒëi·ªÉm**: Nhanh, ƒë∆°n gi·∫£n\n",
        "- **Nh∆∞·ª£c ƒëi·ªÉm**: Ch·ªâ ƒë√°nh gi√° overlap t·ª´ v·ª±ng, kh√¥ng hi·ªÉu semantic\n",
        "\n",
        "### 2.2 ROUGE Score\n",
        "- **ƒê·ªãnh nghƒ©a**: Recall-Oriented Understudy for Gisting Evaluation\n",
        "- **M·ª•c ƒë√≠ch**: ƒê√°nh gi√° ƒë·ªô bao ph·ªß (recall) c·ªßa n·ªôi dung quan tr·ªçng\n",
        "- **C√°c lo·∫°i**: ROUGE-1, ROUGE-2, ROUGE-L\n",
        "- **·ª®ng d·ª•ng**: ƒê·∫∑c bi·ªát hi·ªáu qu·∫£ cho t√≥m t·∫Øt vƒÉn b·∫£n\n",
        "\n",
        "### 2.3 BERTScore\n",
        "- **ƒê·ªãnh nghƒ©a**: Metric d·ª±a tr√™n BERT embeddings\n",
        "- **M·ª•c ƒë√≠ch**: ƒê√°nh gi√° semantic similarity\n",
        "- **∆Øu ƒëi·ªÉm**: Hi·ªÉu ƒë∆∞·ª£c √Ω nghƒ©a, kh√¥ng ch·ªâ t·ª´ v·ª±ng\n",
        "- **Nh∆∞·ª£c ƒëi·ªÉm**: Ch·∫≠m h∆°n, c·∫ßn GPU\n",
        "\n",
        "### 2.4 METEOR\n",
        "- **ƒê·ªãnh nghƒ©a**: Metric for Evaluation of Translation with Explicit Ordering\n",
        "- **∆Øu ƒëi·ªÉm**: Xem x√©t synonyms, correlation cao v·ªõi human judgment (0.96)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "%pip install transformers datasets evaluate torch accelerate\n",
        "%pip install rouge_score bert_score sacrebleu\n",
        "%pip install sentencepiece protobuf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import c√°c th∆∞ vi·ªán\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from transformers import (\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "from datasets import load_metric\n",
        "import evaluate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Thi·∫øt l·∫≠p device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Chu·∫©n b·ªã D·ªØ li·ªáu {#du-lieu}\n",
        "\n",
        "Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ file `all_exercises.json` c√≥ s·∫µn trong th∆∞ m·ª•c.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON\n",
        "with open('all_exercises.json', 'r', encoding='utf-8') as f:\n",
        "    exercises_data = json.load(f)\n",
        "\n",
        "print(f\"T·ªïng s·ªë exercises: {len(exercises_data)}\")\n",
        "print(f\"\\nV√≠ d·ª• m·ªôt exercise:\")\n",
        "print(json.dumps(exercises_data[0], indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ph√¢n t√≠ch d·ªØ li·ªáu\n",
        "df = pd.DataFrame(exercises_data)\n",
        "\n",
        "print(\"Th·ªëng k√™ v·ªÅ types c·ªßa exercises:\")\n",
        "print(df['type'].value_counts())\n",
        "print(\"\\nTh·ªëng k√™ v·ªÅ skills:\")\n",
        "print(df['skill'].value_counts())\n",
        "print(\"\\nTh·ªëng k√™ v·ªÅ levels:\")\n",
        "print(df['level'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o dataset cho task text generation\n",
        "# Ch√∫ng ta s·∫Ω t·∫°o task: t·ª´ c√¢u h·ªèi + options -> t·∫°o explanation\n",
        "\n",
        "def prepare_training_data(exercises_data):\n",
        "    training_examples = []\n",
        "    \n",
        "    for exercise in exercises_data:\n",
        "        if 'options' in exercise and 'explanation' in exercise:\n",
        "            # T·∫°o input text\n",
        "            input_text = f\"Question: {exercise['question']}\\n\"\n",
        "            \n",
        "            if exercise['options']:\n",
        "                input_text += \"Options:\\n\"\n",
        "                for option in exercise['options']:\n",
        "                    input_text += f\"{option['key']}: {option['option']}\\n\"\n",
        "            \n",
        "            input_text += f\"Correct Answer: {exercise['system_answer']}\\n\"\n",
        "            input_text += \"Explain why this answer is correct:\"\n",
        "            \n",
        "            # Target text l√† explanation\n",
        "            target_text = exercise['explanation']\n",
        "            \n",
        "            training_examples.append({\n",
        "                'input_text': input_text,\n",
        "                'target_text': target_text,\n",
        "                'level': exercise.get('level', 'unknown'),\n",
        "                'skill': exercise.get('skill', 'unknown')\n",
        "            })\n",
        "    \n",
        "    return training_examples\n",
        "\n",
        "training_data = prepare_training_data(exercises_data)\n",
        "print(f\"T·ªïng s·ªë training examples: {len(training_data)}\")\n",
        "print(f\"\\nV√≠ d·ª• training example:\")\n",
        "print(f\"Input: {training_data[0]['input_text'][:200]}...\")\n",
        "print(f\"Target: {training_data[0]['target_text'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chia d·ªØ li·ªáu train/validation/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Chia 80% train, 10% validation, 10% test\n",
        "train_data, temp_data = train_test_split(training_data, test_size=0.2, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Train set: {len(train_data)} examples\")\n",
        "print(f\"Validation set: {len(val_data)} examples\")\n",
        "print(f\"Test set: {len(test_data)} examples\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Load v√† Chu·∫©n b·ªã M√¥ h√¨nh T5 {#training}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model v√† tokenizer\n",
        "model_name = \"google/flan-t5-small\"  # S·ª≠ d·ª•ng flan-t5-small cho demo\n",
        "# B·∫°n c√≥ th·ªÉ thay b·∫±ng \"venify/googleT5\" n·∫øu c√≥ access\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load c√°c metrics\n",
        "def load_evaluation_metrics():\n",
        "    metrics = {}\n",
        "    \n",
        "    try:\n",
        "        metrics['bleu'] = evaluate.load('bleu')\n",
        "        print(\"‚úì BLEU metric loaded\")\n",
        "    except:\n",
        "        print(\"‚úó Failed to load BLEU metric\")\n",
        "    \n",
        "    try:\n",
        "        metrics['rouge'] = evaluate.load('rouge')\n",
        "        print(\"‚úì ROUGE metric loaded\")\n",
        "    except:\n",
        "        print(\"‚úó Failed to load ROUGE metric\")\n",
        "    \n",
        "    try:\n",
        "        metrics['bertscore'] = evaluate.load('bertscore')\n",
        "        print(\"‚úì BERTScore metric loaded\")\n",
        "    except:\n",
        "        print(\"‚úó Failed to load BERTScore metric\")\n",
        "    \n",
        "    try:\n",
        "        metrics['meteor'] = evaluate.load('meteor')\n",
        "        print(\"‚úì METEOR metric loaded\")\n",
        "    except:\n",
        "        print(\"‚úó Failed to load METEOR metric\")\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "evaluation_metrics = load_evaluation_metrics()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H√†m generate text\n",
        "def generate_text(model, tokenizer, input_text, max_length=200):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(\n",
        "            input_text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "        \n",
        "        # Generate\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            do_sample=False\n",
        "        )\n",
        "        \n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_text\n",
        "\n",
        "# Test generation v·ªõi m·ªôt v√≠ d·ª•\n",
        "test_input = \"Question: What is the capital of France?\\nOptions:\\nA: London\\nB: Paris\\nC: Berlin\\nD: Madrid\\nCorrect Answer: B\\nExplain why this answer is correct:\"\n",
        "\n",
        "generated = generate_text(model, tokenizer, test_input)\n",
        "\n",
        "print(\"Input:\")\n",
        "print(test_input)\n",
        "print(\"\\nGenerated:\")\n",
        "print(generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ƒê√°nh gi√° tr√™n test set (sample nh·ªè ƒë·ªÉ demo)\n",
        "def evaluate_model(model, tokenizer, test_data, metrics, sample_size=20):\n",
        "    \"\"\"\n",
        "    ƒê√°nh gi√° m√¥ h√¨nh v·ªõi c√°c metrics kh√°c nhau\n",
        "    \"\"\"\n",
        "    # L·∫•y sample ƒë·ªÉ ƒë√°nh gi√° nhanh\n",
        "    sample_data = test_data[:sample_size]\n",
        "    \n",
        "    predictions = []\n",
        "    references = []\n",
        "    \n",
        "    print(f\"Generating predictions for {len(sample_data)} examples...\")\n",
        "    \n",
        "    for i, example in enumerate(sample_data):\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Progress: {i}/{len(sample_data)}\")\n",
        "        \n",
        "        generated = generate_text(model, tokenizer, example['input_text'])\n",
        "        predictions.append(generated)\n",
        "        references.append(example['target_text'])\n",
        "    \n",
        "    print(\"Calculating metrics...\")\n",
        "    results = {}\n",
        "    \n",
        "    # BLEU Score\n",
        "    if 'bleu' in metrics:\n",
        "        try:\n",
        "            bleu_result = metrics['bleu'].compute(\n",
        "                predictions=[pred.split() for pred in predictions],\n",
        "                references=[[ref.split()] for ref in references]\n",
        "            )\n",
        "            results['bleu'] = bleu_result['bleu']\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating BLEU: {e}\")\n",
        "            results['bleu'] = None\n",
        "    \n",
        "    # ROUGE Score\n",
        "    if 'rouge' in metrics:\n",
        "        try:\n",
        "            rouge_result = metrics['rouge'].compute(\n",
        "                predictions=predictions,\n",
        "                references=references\n",
        "            )\n",
        "            results['rouge'] = {\n",
        "                'rouge1': rouge_result['rouge1'],\n",
        "                'rouge2': rouge_result['rouge2'],\n",
        "                'rougeL': rouge_result['rougeL']\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating ROUGE: {e}\")\n",
        "            results['rouge'] = None\n",
        "    \n",
        "    return results, predictions, references\n",
        "\n",
        "# Ch·∫°y evaluation v·ªõi sample nh·ªè ƒë·ªÉ demo\n",
        "if len(training_data) > 0:\n",
        "    # S·ª≠ d·ª•ng sample data ƒë·ªÉ test\n",
        "    sample_test_data = [\n",
        "        {\n",
        "            'input_text': \"Question: What is the capital of France?\\nOptions:\\nA: London\\nB: Paris\\nC: Berlin\\nD: Madrid\\nCorrect Answer: B\\nExplain why this answer is correct:\",\n",
        "            'target_text': \"Paris is the capital and largest city of France. It has been the capital since the late 10th century and is the political, economic, and cultural center of the country.\"\n",
        "        },\n",
        "        {\n",
        "            'input_text': \"Question: Which tense is used in 'I have been studying'?\\nOptions:\\nA: Present Simple\\nB: Present Perfect\\nC: Present Perfect Continuous\\nD: Past Perfect\\nCorrect Answer: C\\nExplain why this answer is correct:\",\n",
        "            'target_text': \"The phrase 'I have been studying' uses the Present Perfect Continuous tense. This tense is formed with 'have/has + been + verb-ing' and indicates an action that started in the past and continues to the present moment or has recently finished with relevance to the present.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    eval_results, predictions, references = evaluate_model(\n",
        "        model, tokenizer, sample_test_data, evaluation_metrics, sample_size=2\n",
        "    )\n",
        "else:\n",
        "    print(\"No training data available for evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£ ƒë√°nh gi√°\n",
        "def display_evaluation_results(results):\n",
        "    print(\"=\" * 50)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if results.get('bleu') is not None:\n",
        "        print(f\"üìä BLEU Score: {results['bleu']:.4f}\")\n",
        "        print(\"   - Measures precision of n-gram overlaps\")\n",
        "        print(\"   - Range: 0-1 (higher is better)\")\n",
        "        print()\n",
        "    \n",
        "    if results.get('rouge') is not None:\n",
        "        print(f\"üìä ROUGE Scores:\")\n",
        "        print(f\"   - ROUGE-1: {results['rouge']['rouge1']:.4f}\")\n",
        "        print(f\"   - ROUGE-2: {results['rouge']['rouge2']:.4f}\")\n",
        "        print(f\"   - ROUGE-L: {results['rouge']['rougeL']:.4f}\")\n",
        "        print(\"   - Measures recall of important content\")\n",
        "        print()\n",
        "    \n",
        "    # Hi·ªÉn th·ªã so s√°nh predictions vs references\n",
        "    print(\"üìù SAMPLE COMPARISONS\")\n",
        "    print(\"=\" * 30)\n",
        "    if 'predictions' in globals() and 'references' in globals():\n",
        "        for i in range(min(2, len(predictions))):\n",
        "            print(f\"\\nExample {i+1}:\")\n",
        "            print(f\"Reference: {references[i][:150]}...\")\n",
        "            print(f\"Predicted: {predictions[i][:150]}...\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "if 'eval_results' in globals():\n",
        "    display_evaluation_results(eval_results)\n",
        "else:\n",
        "    print(\"Run evaluation first to see results\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Visualization v√† Ph√¢n t√≠ch Chi ti·∫øt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization c·ªßa k·∫øt qu·∫£\n",
        "def plot_evaluation_results(results):\n",
        "    if not results:\n",
        "        print(\"No results to plot\")\n",
        "        return\n",
        "        \n",
        "    # Chu·∫©n b·ªã d·ªØ li·ªáu cho visualization\n",
        "    metrics_data = []\n",
        "    \n",
        "    if results.get('bleu') is not None:\n",
        "        metrics_data.append(('BLEU', results['bleu']))\n",
        "    \n",
        "    if results.get('rouge') is not None:\n",
        "        metrics_data.extend([\n",
        "            ('ROUGE-1', results['rouge']['rouge1']),\n",
        "            ('ROUGE-2', results['rouge']['rouge2']),\n",
        "            ('ROUGE-L', results['rouge']['rougeL'])\n",
        "        ])\n",
        "    \n",
        "    if metrics_data:\n",
        "        # T·∫°o bar chart\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        metrics_names = [item[0] for item in metrics_data]\n",
        "        scores = [item[1] for item in metrics_data]\n",
        "        \n",
        "        bars = plt.bar(metrics_names, scores, \n",
        "                      color=['skyblue', 'lightcoral', 'lightgreen', 'orange', 'purple'][:len(metrics_names)])\n",
        "        \n",
        "        # Th√™m gi√° tr·ªã l√™n ƒë·∫ßu bar\n",
        "        for bar, score in zip(bars, scores):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "        \n",
        "        plt.title('Model Evaluation Results', fontsize=16, fontweight='bold')\n",
        "        plt.ylabel('Score', fontsize=12)\n",
        "        plt.xlabel('Metrics', fontsize=12)\n",
        "        plt.ylim(0, 1)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No metrics data available for plotting\")\n",
        "\n",
        "# T·∫°o bi·ªÉu ƒë·ªì so s√°nh\n",
        "def create_comparison_table():\n",
        "    \"\"\"\n",
        "    T·∫°o b·∫£ng so s√°nh c√°c metrics\n",
        "    \"\"\"\n",
        "    comparison_data = {\n",
        "        'Metric': ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BERTScore', 'METEOR'],\n",
        "        'Purpose': [\n",
        "            'Precision-based n-gram overlap',\n",
        "            'Recall-based unigram overlap', \n",
        "            'Recall-based bigram overlap',\n",
        "            'Longest common subsequence',\n",
        "            'Semantic similarity via BERT',\n",
        "            'Harmonic mean + synonyms'\n",
        "        ],\n",
        "        'Range': ['0-1', '0-1', '0-1', '0-1', '0-1', '0-1'],\n",
        "        'Best For': [\n",
        "            'Machine Translation',\n",
        "            'Text Summarization',\n",
        "            'Text Summarization',\n",
        "            'Text Summarization',\n",
        "            'Any text generation',\n",
        "            'Machine Translation'\n",
        "        ],\n",
        "        'Speed': ['Fast', 'Fast', 'Fast', 'Fast', 'Slow', 'Medium']\n",
        "    }\n",
        "    \n",
        "    df_comparison = pd.DataFrame(comparison_data)\n",
        "    print(\"üìä METRICS COMPARISON TABLE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(df_comparison.to_string(index=False))\n",
        "    return df_comparison\n",
        "\n",
        "# G·ªçi functions\n",
        "if 'eval_results' in globals():\n",
        "    plot_evaluation_results(eval_results)\n",
        "\n",
        "create_comparison_table()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Training M√¥ h√¨nh (Optional)\n",
        "\n",
        "‚ö†Ô∏è **L∆∞u √Ω**: Training m√¥ h√¨nh T5 c·∫ßn nhi·ªÅu t√†i nguy√™n GPU v√† th·ªùi gian. Ph·∫ßn n√†y ch·ªâ ƒë·ªÉ tham kh·∫£o.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o custom dataset class cho training\n",
        "class T5Dataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        # Tokenize input\n",
        "        input_encoding = self.tokenizer(\n",
        "            item['input_text'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # Tokenize target\n",
        "        target_encoding = self.tokenizer(\n",
        "            item['target_text'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': input_encoding['input_ids'].flatten(),\n",
        "            'attention_mask': input_encoding['attention_mask'].flatten(),\n",
        "            'labels': target_encoding['input_ids'].flatten()\n",
        "        }\n",
        "\n",
        "# Thi·∫øt l·∫≠p training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,  # Nh·ªè ƒë·ªÉ tr√°nh OOM\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "print(\"Training setup ready!\")\n",
        "print(\"Uncomment the training code below to start training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training code (UNCOMMENT ƒë·ªÉ train)\n",
        "\"\"\"\n",
        "# T·∫°o datasets cho training\n",
        "if len(training_data) > 0:\n",
        "    train_dataset = T5Dataset(train_data, tokenizer)\n",
        "    val_dataset = T5Dataset(val_data, tokenizer)\n",
        "    \n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    \n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    print(\"Training completed!\")\n",
        "    \n",
        "    # Save the model\n",
        "    trainer.save_model(\"./fine_tuned_t5\")\n",
        "    print(\"Model saved to ./fine_tuned_t5\")\n",
        "else:\n",
        "    print(\"No training data available\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training code is commented out\")\n",
        "print(\"Uncomment the code above to start training\")\n",
        "print(\"‚ö†Ô∏è Warning: Training may take several hours and requires GPU\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Khuy·∫øn ngh·ªã v√† Best Practices\n",
        "\n",
        "### 7.1 L·ª±a ch·ªçn Metrics ph√π h·ª£p\n",
        "\n",
        "**Cho Machine Translation:**\n",
        "- BLEU: Metric truy·ªÅn th·ªëng, t·ªët cho so s√°nh\n",
        "- BERTScore: T·ªët h∆°n cho semantic similarity\n",
        "- COMET: State-of-the-art cho translation\n",
        "\n",
        "**Cho Text Summarization:**\n",
        "- ROUGE: Metric chu·∫©n cho summarization\n",
        "- BERTScore: ƒê√°nh gi√° semantic content\n",
        "\n",
        "**Cho Text Generation t·ªïng qu√°t:**\n",
        "- Combination of multiple metrics\n",
        "- Human evaluation cho quality check\n",
        "\n",
        "### 7.2 C·∫£i thi·ªán Model Performance\n",
        "\n",
        "1. **Data Quality**: ƒê·∫£m b·∫£o training data ch·∫•t l∆∞·ª£ng cao\n",
        "2. **Hyperparameter Tuning**: Learning rate, batch size, epochs\n",
        "3. **Model Architecture**: Th·ª≠ c√°c variants kh√°c nhau c·ªßa T5\n",
        "4. **Post-processing**: Length control, repetition penalty\n",
        "\n",
        "### 7.3 Monitoring v√† Continuous Improvement\n",
        "\n",
        "1. **Regular Evaluation**: ƒê√°nh gi√° ƒë·ªãnh k·ª≥ v·ªõi test set m·ªõi\n",
        "2. **A/B Testing**: So s√°nh v·ªõi baseline models\n",
        "3. **Human Feedback**: K·∫øt h·ª£p human evaluation\n",
        "4. **Domain Adaptation**: Fine-tune cho specific domains\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o evaluation report\n",
        "def create_evaluation_report(results):\n",
        "    report = f\"\"\"\n",
        "# T5 Model Evaluation Report\n",
        "\n",
        "## Model Information\n",
        "- Model: {model_name}\n",
        "- Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "- Purpose: English Learning Exercise Explanation Generation\n",
        "\n",
        "## Metrics Results\n",
        "\"\"\"\n",
        "    \n",
        "    if results.get('bleu') is not None:\n",
        "        report += f\"\\n- BLEU Score: {results['bleu']:.4f}\"\n",
        "    \n",
        "    if results.get('rouge') is not None:\n",
        "        report += f\"\"\"\n",
        "- ROUGE-1: {results['rouge']['rouge1']:.4f}\n",
        "- ROUGE-2: {results['rouge']['rouge2']:.4f}\n",
        "- ROUGE-L: {results['rouge']['rougeL']:.4f}\"\"\"\n",
        "    \n",
        "    report += \"\"\"\n",
        "\n",
        "## Interpretation Guide\n",
        "- **BLEU**: 0.3+ is good, 0.5+ is excellent for translation\n",
        "- **ROUGE-1**: 0.4+ is good for summarization\n",
        "- **ROUGE-2**: 0.2+ is good for summarization\n",
        "- **ROUGE-L**: 0.3+ is good for summarization\n",
        "\n",
        "## Recommendations\n",
        "Based on the evaluation results:\n",
        "\n",
        "1. Consider the semantic metrics (BERTScore) alongside traditional metrics\n",
        "2. Monitor text quality through human evaluation\n",
        "3. Use multiple metrics for comprehensive evaluation\n",
        "4. Regular re-evaluation with fresh data\n",
        "5. Focus on educational appropriateness for English learning\n",
        "\n",
        "## venify/googleT5 vs Standard T5\n",
        "- venify/googleT5 is likely fine-tuned for educational content\n",
        "- Should show better performance on English learning tasks\n",
        "- May have specialized vocabulary for educational contexts\n",
        "\"\"\"\n",
        "    \n",
        "    return report\n",
        "\n",
        "# T·∫°o report\n",
        "if 'eval_results' in globals() and eval_results:\n",
        "    evaluation_report = create_evaluation_report(eval_results)\n",
        "    print(evaluation_report)\n",
        "    \n",
        "    # L∆∞u report v√†o file\n",
        "    with open('t5_evaluation_report.md', 'w', encoding='utf-8') as f:\n",
        "        f.write(evaluation_report)\n",
        "    \n",
        "    print(\"\\n‚úÖ Evaluation report saved to 't5_evaluation_report.md'\")\n",
        "else:\n",
        "    print(\"No evaluation results available. Run the evaluation first.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## K·∫øt lu·∫≠n\n",
        "\n",
        "Notebook n√†y ƒë√£ h∆∞·ªõng d·∫´n b·∫°n:\n",
        "\n",
        "1. **Hi·ªÉu v·ªÅ c√°c metrics ƒë√°nh gi√°**: BLEU, ROUGE, BERTScore, METEOR\n",
        "2. **Chu·∫©n b·ªã d·ªØ li·ªáu**: T·ª´ exercises JSON th√†nh training format\n",
        "3. **Setup v√† ƒë√°nh gi√° m√¥ h√¨nh T5**: V·ªõi Hugging Face Transformers\n",
        "4. **ƒê√°nh gi√° to√†n di·ªán**: Multiple metrics cho comprehensive evaluation\n",
        "5. **Ph√¢n t√≠ch k·∫øt qu·∫£**: Statistical analysis v√† visualization\n",
        "6. **Best practices**: Recommendations cho model improvement\n",
        "\n",
        "### ƒê√°nh gi√° venify/googleT5:\n",
        "\n",
        "**Ph∆∞∆°ng ph√°p ƒë√°nh gi√° ƒë∆∞·ª£c khuy·∫øn ngh·ªã:**\n",
        "\n",
        "1. **ROUGE scores** - Quan tr·ªçng nh·∫•t cho task explanation generation\n",
        "2. **BERTScore** - ƒê√°nh gi√° semantic similarity\n",
        "3. **Human evaluation** - ƒê√°nh gi√° educational appropriateness\n",
        "4. **Domain-specific metrics** - Accuracy of educational content\n",
        "\n",
        "### Next Steps:\n",
        "1. **Load m√¥ h√¨nh venify/googleT5** th·ª±c t·∫ø (n·∫øu c√≥ access)\n",
        "2. **ƒê√°nh gi√° tr√™n dataset l·ªõn h∆°n**\n",
        "3. **So s√°nh v·ªõi baseline models** (standard T5, GPT, etc.)\n",
        "4. **Implement human evaluation** cho educational quality\n",
        "5. **Fine-tune** cho specific English learning tasks\n",
        "\n",
        "### L∆∞u √Ω quan tr·ªçng:\n",
        "- **ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng NLG** kh√¥ng ch·ªâ d·ª±a v√†o metrics\n",
        "- **Human evaluation** c·∫ßn thi·∫øt cho educational content\n",
        "- **Multiple metrics** cung c·∫•p c√°i nh√¨n to√†n di·ªán\n",
        "- **Domain expertise** quan tr·ªçng cho English learning applications\n",
        "\n",
        "**Happy Evaluating! üöÄüìä**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
