{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Hướng dẫn Train và Đánh giá Mô hình venify/googleT5\n",
        "\n",
        "## Mục lục\n",
        "1. [Giới thiệu về mô hình T5](#gioi-thieu)\n",
        "2. [Các metrics đánh giá chất lượng](#metrics)\n",
        "3. [Chuẩn bị dữ liệu](#du-lieu)\n",
        "4. [Training mô hình](#training)\n",
        "5. [Đánh giá chất lượng mô hình](#danh-gia)\n",
        "6. [So sánh kết quả](#so-sanh)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Giới thiệu về mô hình T5 {#gioi-thieu}\n",
        "\n",
        "**T5 (Text-to-Text Transfer Transformer)** là một mô hình ngôn ngữ lớn được phát triển bởi Google. Mô hình này:\n",
        "- Chuyển đổi mọi tác vụ NLP thành định dạng text-to-text\n",
        "- Sử dụng kiến trúc Transformer encoder-decoder\n",
        "- Rất hiệu quả cho các tác vụ như dịch thuật, tóm tắt văn bản, sinh câu hỏi\n",
        "\n",
        "**venify/googleT5** là một biến thể được fine-tune đặc biệt cho các tác vụ giáo dục tiếng Anh.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Các Metrics Đánh Giá Chất Lượng {#metrics}\n",
        "\n",
        "### 2.1 BLEU Score\n",
        "- **Định nghĩa**: Bilingual Evaluation Understudy\n",
        "- **Mục đích**: Đánh giá độ chính xác (precision) của văn bản sinh ra\n",
        "- **Phạm vi**: 0-1 (càng cao càng tốt)\n",
        "- **Ưu điểm**: Nhanh, đơn giản\n",
        "- **Nhược điểm**: Chỉ đánh giá overlap từ vựng, không hiểu semantic\n",
        "\n",
        "### 2.2 ROUGE Score\n",
        "- **Định nghĩa**: Recall-Oriented Understudy for Gisting Evaluation\n",
        "- **Mục đích**: Đánh giá độ bao phủ (recall) của nội dung quan trọng\n",
        "- **Các loại**: ROUGE-1, ROUGE-2, ROUGE-L\n",
        "- **Ứng dụng**: Đặc biệt hiệu quả cho tóm tắt văn bản\n",
        "\n",
        "### 2.3 BERTScore\n",
        "- **Định nghĩa**: Metric dựa trên BERT embeddings\n",
        "- **Mục đích**: Đánh giá semantic similarity\n",
        "- **Ưu điểm**: Hiểu được ý nghĩa, không chỉ từ vựng\n",
        "- **Nhược điểm**: Chậm hơn, cần GPU\n",
        "\n",
        "### 2.4 METEOR\n",
        "- **Định nghĩa**: Metric for Evaluation of Translation with Explicit Ordering\n",
        "- **Ưu điểm**: Xem xét synonyms, correlation cao với human judgment (0.96)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cài đặt các thư viện cần thiết\n",
        "%pip install transformers datasets evaluate torch accelerate\n",
        "%pip install rouge_score bert_score sacrebleu\n",
        "%pip install sentencepiece protobuf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import các thư viện\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from transformers import (\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "from datasets import load_metric\n",
        "import evaluate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Thiết lập device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Chuẩn bị Dữ liệu {#du-lieu}\n",
        "\n",
        "Chúng ta sẽ sử dụng dữ liệu từ file `all_exercises.json` có sẵn trong thư mục.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Đọc dữ liệu từ file JSON\n",
        "with open('all_exercises.json', 'r', encoding='utf-8') as f:\n",
        "    exercises_data = json.load(f)\n",
        "\n",
        "print(f\"Tổng số exercises: {len(exercises_data)}\")\n",
        "print(f\"\\nVí dụ một exercise:\")\n",
        "print(json.dumps(exercises_data[0], indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phân tích dữ liệu\n",
        "df = pd.DataFrame(exercises_data)\n",
        "\n",
        "print(\"Thống kê về types của exercises:\")\n",
        "print(df['type'].value_counts())\n",
        "print(\"\\nThống kê về skills:\")\n",
        "print(df['skill'].value_counts())\n",
        "print(\"\\nThống kê về levels:\")\n",
        "print(df['level'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tạo dataset cho task text generation\n",
        "# Chúng ta sẽ tạo task: từ câu hỏi + options -> tạo explanation\n",
        "\n",
        "def prepare_training_data(exercises_data):\n",
        "    training_examples = []\n",
        "    \n",
        "    for exercise in exercises_data:\n",
        "        if 'options' in exercise and 'explanation' in exercise:\n",
        "            # Tạo input text\n",
        "            input_text = f\"Question: {exercise['question']}\\n\"\n",
        "            \n",
        "            if exercise['options']:\n",
        "                input_text += \"Options:\\n\"\n",
        "                for option in exercise['options']:\n",
        "                    input_text += f\"{option['key']}: {option['option']}\\n\"\n",
        "            \n",
        "            input_text += f\"Correct Answer: {exercise['system_answer']}\\n\"\n",
        "            input_text += \"Explain why this answer is correct:\"\n",
        "            \n",
        "            # Target text là explanation\n",
        "            target_text = exercise['explanation']\n",
        "            \n",
        "            training_examples.append({\n",
        "                'input_text': input_text,\n",
        "                'target_text': target_text,\n",
        "                'level': exercise.get('level', 'unknown'),\n",
        "                'skill': exercise.get('skill', 'unknown')\n",
        "            })\n",
        "    \n",
        "    return training_examples\n",
        "\n",
        "training_data = prepare_training_data(exercises_data)\n",
        "print(f\"Tổng số training examples: {len(training_data)}\")\n",
        "print(f\"\\nVí dụ training example:\")\n",
        "print(f\"Input: {training_data[0]['input_text'][:200]}...\")\n",
        "print(f\"Target: {training_data[0]['target_text'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chia dữ liệu train/validation/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Chia 80% train, 10% validation, 10% test\n",
        "train_data, temp_data = train_test_split(training_data, test_size=0.2, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Train set: {len(train_data)} examples\")\n",
        "print(f\"Validation set: {len(val_data)} examples\")\n",
        "print(f\"Test set: {len(test_data)} examples\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Load và Chuẩn bị Mô hình T5 {#training}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model và tokenizer\n",
        "model_name = \"google/flan-t5-small\"  # Sử dụng flan-t5-small cho demo\n",
        "# Bạn có thể thay bằng \"venify/googleT5\" nếu có access\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load các metrics\n",
        "def load_evaluation_metrics():\n",
        "    metrics = {}\n",
        "    \n",
        "    try:\n",
        "        metrics['bleu'] = evaluate.load('bleu')\n",
        "        print(\"✓ BLEU metric loaded\")\n",
        "    except:\n",
        "        print(\"✗ Failed to load BLEU metric\")\n",
        "    \n",
        "    try:\n",
        "        metrics['rouge'] = evaluate.load('rouge')\n",
        "        print(\"✓ ROUGE metric loaded\")\n",
        "    except:\n",
        "        print(\"✗ Failed to load ROUGE metric\")\n",
        "    \n",
        "    try:\n",
        "        metrics['bertscore'] = evaluate.load('bertscore')\n",
        "        print(\"✓ BERTScore metric loaded\")\n",
        "    except:\n",
        "        print(\"✗ Failed to load BERTScore metric\")\n",
        "    \n",
        "    try:\n",
        "        metrics['meteor'] = evaluate.load('meteor')\n",
        "        print(\"✓ METEOR metric loaded\")\n",
        "    except:\n",
        "        print(\"✗ Failed to load METEOR metric\")\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "evaluation_metrics = load_evaluation_metrics()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hàm generate text\n",
        "def generate_text(model, tokenizer, input_text, max_length=200):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(\n",
        "            input_text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "        \n",
        "        # Generate\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            do_sample=False\n",
        "        )\n",
        "        \n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_text\n",
        "\n",
        "# Test generation với một ví dụ\n",
        "test_input = \"Question: What is the capital of France?\\nOptions:\\nA: London\\nB: Paris\\nC: Berlin\\nD: Madrid\\nCorrect Answer: B\\nExplain why this answer is correct:\"\n",
        "\n",
        "generated = generate_text(model, tokenizer, test_input)\n",
        "\n",
        "print(\"Input:\")\n",
        "print(test_input)\n",
        "print(\"\\nGenerated:\")\n",
        "print(generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Đánh giá trên test set (sample nhỏ để demo)\n",
        "def evaluate_model(model, tokenizer, test_data, metrics, sample_size=20):\n",
        "    \"\"\"\n",
        "    Đánh giá mô hình với các metrics khác nhau\n",
        "    \"\"\"\n",
        "    # Lấy sample để đánh giá nhanh\n",
        "    sample_data = test_data[:sample_size]\n",
        "    \n",
        "    predictions = []\n",
        "    references = []\n",
        "    \n",
        "    print(f\"Generating predictions for {len(sample_data)} examples...\")\n",
        "    \n",
        "    for i, example in enumerate(sample_data):\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Progress: {i}/{len(sample_data)}\")\n",
        "        \n",
        "        generated = generate_text(model, tokenizer, example['input_text'])\n",
        "        predictions.append(generated)\n",
        "        references.append(example['target_text'])\n",
        "    \n",
        "    print(\"Calculating metrics...\")\n",
        "    results = {}\n",
        "    \n",
        "    # BLEU Score\n",
        "    if 'bleu' in metrics:\n",
        "        try:\n",
        "            bleu_result = metrics['bleu'].compute(\n",
        "                predictions=[pred.split() for pred in predictions],\n",
        "                references=[[ref.split()] for ref in references]\n",
        "            )\n",
        "            results['bleu'] = bleu_result['bleu']\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating BLEU: {e}\")\n",
        "            results['bleu'] = None\n",
        "    \n",
        "    # ROUGE Score\n",
        "    if 'rouge' in metrics:\n",
        "        try:\n",
        "            rouge_result = metrics['rouge'].compute(\n",
        "                predictions=predictions,\n",
        "                references=references\n",
        "            )\n",
        "            results['rouge'] = {\n",
        "                'rouge1': rouge_result['rouge1'],\n",
        "                'rouge2': rouge_result['rouge2'],\n",
        "                'rougeL': rouge_result['rougeL']\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating ROUGE: {e}\")\n",
        "            results['rouge'] = None\n",
        "    \n",
        "    return results, predictions, references\n",
        "\n",
        "# Chạy evaluation với sample nhỏ để demo\n",
        "if len(training_data) > 0:\n",
        "    # Sử dụng sample data để test\n",
        "    sample_test_data = [\n",
        "        {\n",
        "            'input_text': \"Question: What is the capital of France?\\nOptions:\\nA: London\\nB: Paris\\nC: Berlin\\nD: Madrid\\nCorrect Answer: B\\nExplain why this answer is correct:\",\n",
        "            'target_text': \"Paris is the capital and largest city of France. It has been the capital since the late 10th century and is the political, economic, and cultural center of the country.\"\n",
        "        },\n",
        "        {\n",
        "            'input_text': \"Question: Which tense is used in 'I have been studying'?\\nOptions:\\nA: Present Simple\\nB: Present Perfect\\nC: Present Perfect Continuous\\nD: Past Perfect\\nCorrect Answer: C\\nExplain why this answer is correct:\",\n",
        "            'target_text': \"The phrase 'I have been studying' uses the Present Perfect Continuous tense. This tense is formed with 'have/has + been + verb-ing' and indicates an action that started in the past and continues to the present moment or has recently finished with relevance to the present.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    eval_results, predictions, references = evaluate_model(\n",
        "        model, tokenizer, sample_test_data, evaluation_metrics, sample_size=2\n",
        "    )\n",
        "else:\n",
        "    print(\"No training data available for evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hiển thị kết quả đánh giá\n",
        "def display_evaluation_results(results):\n",
        "    print(\"=\" * 50)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if results.get('bleu') is not None:\n",
        "        print(f\"📊 BLEU Score: {results['bleu']:.4f}\")\n",
        "        print(\"   - Measures precision of n-gram overlaps\")\n",
        "        print(\"   - Range: 0-1 (higher is better)\")\n",
        "        print()\n",
        "    \n",
        "    if results.get('rouge') is not None:\n",
        "        print(f\"📊 ROUGE Scores:\")\n",
        "        print(f\"   - ROUGE-1: {results['rouge']['rouge1']:.4f}\")\n",
        "        print(f\"   - ROUGE-2: {results['rouge']['rouge2']:.4f}\")\n",
        "        print(f\"   - ROUGE-L: {results['rouge']['rougeL']:.4f}\")\n",
        "        print(\"   - Measures recall of important content\")\n",
        "        print()\n",
        "    \n",
        "    # Hiển thị so sánh predictions vs references\n",
        "    print(\"📝 SAMPLE COMPARISONS\")\n",
        "    print(\"=\" * 30)\n",
        "    if 'predictions' in globals() and 'references' in globals():\n",
        "        for i in range(min(2, len(predictions))):\n",
        "            print(f\"\\nExample {i+1}:\")\n",
        "            print(f\"Reference: {references[i][:150]}...\")\n",
        "            print(f\"Predicted: {predictions[i][:150]}...\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "if 'eval_results' in globals():\n",
        "    display_evaluation_results(eval_results)\n",
        "else:\n",
        "    print(\"Run evaluation first to see results\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Visualization và Phân tích Chi tiết\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization của kết quả\n",
        "def plot_evaluation_results(results):\n",
        "    if not results:\n",
        "        print(\"No results to plot\")\n",
        "        return\n",
        "        \n",
        "    # Chuẩn bị dữ liệu cho visualization\n",
        "    metrics_data = []\n",
        "    \n",
        "    if results.get('bleu') is not None:\n",
        "        metrics_data.append(('BLEU', results['bleu']))\n",
        "    \n",
        "    if results.get('rouge') is not None:\n",
        "        metrics_data.extend([\n",
        "            ('ROUGE-1', results['rouge']['rouge1']),\n",
        "            ('ROUGE-2', results['rouge']['rouge2']),\n",
        "            ('ROUGE-L', results['rouge']['rougeL'])\n",
        "        ])\n",
        "    \n",
        "    if metrics_data:\n",
        "        # Tạo bar chart\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        metrics_names = [item[0] for item in metrics_data]\n",
        "        scores = [item[1] for item in metrics_data]\n",
        "        \n",
        "        bars = plt.bar(metrics_names, scores, \n",
        "                      color=['skyblue', 'lightcoral', 'lightgreen', 'orange', 'purple'][:len(metrics_names)])\n",
        "        \n",
        "        # Thêm giá trị lên đầu bar\n",
        "        for bar, score in zip(bars, scores):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "        \n",
        "        plt.title('Model Evaluation Results', fontsize=16, fontweight='bold')\n",
        "        plt.ylabel('Score', fontsize=12)\n",
        "        plt.xlabel('Metrics', fontsize=12)\n",
        "        plt.ylim(0, 1)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No metrics data available for plotting\")\n",
        "\n",
        "# Tạo biểu đồ so sánh\n",
        "def create_comparison_table():\n",
        "    \"\"\"\n",
        "    Tạo bảng so sánh các metrics\n",
        "    \"\"\"\n",
        "    comparison_data = {\n",
        "        'Metric': ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BERTScore', 'METEOR'],\n",
        "        'Purpose': [\n",
        "            'Precision-based n-gram overlap',\n",
        "            'Recall-based unigram overlap', \n",
        "            'Recall-based bigram overlap',\n",
        "            'Longest common subsequence',\n",
        "            'Semantic similarity via BERT',\n",
        "            'Harmonic mean + synonyms'\n",
        "        ],\n",
        "        'Range': ['0-1', '0-1', '0-1', '0-1', '0-1', '0-1'],\n",
        "        'Best For': [\n",
        "            'Machine Translation',\n",
        "            'Text Summarization',\n",
        "            'Text Summarization',\n",
        "            'Text Summarization',\n",
        "            'Any text generation',\n",
        "            'Machine Translation'\n",
        "        ],\n",
        "        'Speed': ['Fast', 'Fast', 'Fast', 'Fast', 'Slow', 'Medium']\n",
        "    }\n",
        "    \n",
        "    df_comparison = pd.DataFrame(comparison_data)\n",
        "    print(\"📊 METRICS COMPARISON TABLE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(df_comparison.to_string(index=False))\n",
        "    return df_comparison\n",
        "\n",
        "# Gọi functions\n",
        "if 'eval_results' in globals():\n",
        "    plot_evaluation_results(eval_results)\n",
        "\n",
        "create_comparison_table()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Training Mô hình (Optional)\n",
        "\n",
        "⚠️ **Lưu ý**: Training mô hình T5 cần nhiều tài nguyên GPU và thời gian. Phần này chỉ để tham khảo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tạo custom dataset class cho training\n",
        "class T5Dataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        # Tokenize input\n",
        "        input_encoding = self.tokenizer(\n",
        "            item['input_text'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # Tokenize target\n",
        "        target_encoding = self.tokenizer(\n",
        "            item['target_text'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': input_encoding['input_ids'].flatten(),\n",
        "            'attention_mask': input_encoding['attention_mask'].flatten(),\n",
        "            'labels': target_encoding['input_ids'].flatten()\n",
        "        }\n",
        "\n",
        "# Thiết lập training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,  # Nhỏ để tránh OOM\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "print(\"Training setup ready!\")\n",
        "print(\"Uncomment the training code below to start training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training code (UNCOMMENT để train)\n",
        "\"\"\"\n",
        "# Tạo datasets cho training\n",
        "if len(training_data) > 0:\n",
        "    train_dataset = T5Dataset(train_data, tokenizer)\n",
        "    val_dataset = T5Dataset(val_data, tokenizer)\n",
        "    \n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    \n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    print(\"Training completed!\")\n",
        "    \n",
        "    # Save the model\n",
        "    trainer.save_model(\"./fine_tuned_t5\")\n",
        "    print(\"Model saved to ./fine_tuned_t5\")\n",
        "else:\n",
        "    print(\"No training data available\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training code is commented out\")\n",
        "print(\"Uncomment the code above to start training\")\n",
        "print(\"⚠️ Warning: Training may take several hours and requires GPU\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Khuyến nghị và Best Practices\n",
        "\n",
        "### 7.1 Lựa chọn Metrics phù hợp\n",
        "\n",
        "**Cho Machine Translation:**\n",
        "- BLEU: Metric truyền thống, tốt cho so sánh\n",
        "- BERTScore: Tốt hơn cho semantic similarity\n",
        "- COMET: State-of-the-art cho translation\n",
        "\n",
        "**Cho Text Summarization:**\n",
        "- ROUGE: Metric chuẩn cho summarization\n",
        "- BERTScore: Đánh giá semantic content\n",
        "\n",
        "**Cho Text Generation tổng quát:**\n",
        "- Combination of multiple metrics\n",
        "- Human evaluation cho quality check\n",
        "\n",
        "### 7.2 Cải thiện Model Performance\n",
        "\n",
        "1. **Data Quality**: Đảm bảo training data chất lượng cao\n",
        "2. **Hyperparameter Tuning**: Learning rate, batch size, epochs\n",
        "3. **Model Architecture**: Thử các variants khác nhau của T5\n",
        "4. **Post-processing**: Length control, repetition penalty\n",
        "\n",
        "### 7.3 Monitoring và Continuous Improvement\n",
        "\n",
        "1. **Regular Evaluation**: Đánh giá định kỳ với test set mới\n",
        "2. **A/B Testing**: So sánh với baseline models\n",
        "3. **Human Feedback**: Kết hợp human evaluation\n",
        "4. **Domain Adaptation**: Fine-tune cho specific domains\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tạo evaluation report\n",
        "def create_evaluation_report(results):\n",
        "    report = f\"\"\"\n",
        "# T5 Model Evaluation Report\n",
        "\n",
        "## Model Information\n",
        "- Model: {model_name}\n",
        "- Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "- Purpose: English Learning Exercise Explanation Generation\n",
        "\n",
        "## Metrics Results\n",
        "\"\"\"\n",
        "    \n",
        "    if results.get('bleu') is not None:\n",
        "        report += f\"\\n- BLEU Score: {results['bleu']:.4f}\"\n",
        "    \n",
        "    if results.get('rouge') is not None:\n",
        "        report += f\"\"\"\n",
        "- ROUGE-1: {results['rouge']['rouge1']:.4f}\n",
        "- ROUGE-2: {results['rouge']['rouge2']:.4f}\n",
        "- ROUGE-L: {results['rouge']['rougeL']:.4f}\"\"\"\n",
        "    \n",
        "    report += \"\"\"\n",
        "\n",
        "## Interpretation Guide\n",
        "- **BLEU**: 0.3+ is good, 0.5+ is excellent for translation\n",
        "- **ROUGE-1**: 0.4+ is good for summarization\n",
        "- **ROUGE-2**: 0.2+ is good for summarization\n",
        "- **ROUGE-L**: 0.3+ is good for summarization\n",
        "\n",
        "## Recommendations\n",
        "Based on the evaluation results:\n",
        "\n",
        "1. Consider the semantic metrics (BERTScore) alongside traditional metrics\n",
        "2. Monitor text quality through human evaluation\n",
        "3. Use multiple metrics for comprehensive evaluation\n",
        "4. Regular re-evaluation with fresh data\n",
        "5. Focus on educational appropriateness for English learning\n",
        "\n",
        "## venify/googleT5 vs Standard T5\n",
        "- venify/googleT5 is likely fine-tuned for educational content\n",
        "- Should show better performance on English learning tasks\n",
        "- May have specialized vocabulary for educational contexts\n",
        "\"\"\"\n",
        "    \n",
        "    return report\n",
        "\n",
        "# Tạo report\n",
        "if 'eval_results' in globals() and eval_results:\n",
        "    evaluation_report = create_evaluation_report(eval_results)\n",
        "    print(evaluation_report)\n",
        "    \n",
        "    # Lưu report vào file\n",
        "    with open('t5_evaluation_report.md', 'w', encoding='utf-8') as f:\n",
        "        f.write(evaluation_report)\n",
        "    \n",
        "    print(\"\\n✅ Evaluation report saved to 't5_evaluation_report.md'\")\n",
        "else:\n",
        "    print(\"No evaluation results available. Run the evaluation first.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Kết luận\n",
        "\n",
        "Notebook này đã hướng dẫn bạn:\n",
        "\n",
        "1. **Hiểu về các metrics đánh giá**: BLEU, ROUGE, BERTScore, METEOR\n",
        "2. **Chuẩn bị dữ liệu**: Từ exercises JSON thành training format\n",
        "3. **Setup và đánh giá mô hình T5**: Với Hugging Face Transformers\n",
        "4. **Đánh giá toàn diện**: Multiple metrics cho comprehensive evaluation\n",
        "5. **Phân tích kết quả**: Statistical analysis và visualization\n",
        "6. **Best practices**: Recommendations cho model improvement\n",
        "\n",
        "### Đánh giá venify/googleT5:\n",
        "\n",
        "**Phương pháp đánh giá được khuyến nghị:**\n",
        "\n",
        "1. **ROUGE scores** - Quan trọng nhất cho task explanation generation\n",
        "2. **BERTScore** - Đánh giá semantic similarity\n",
        "3. **Human evaluation** - Đánh giá educational appropriateness\n",
        "4. **Domain-specific metrics** - Accuracy of educational content\n",
        "\n",
        "### Next Steps:\n",
        "1. **Load mô hình venify/googleT5** thực tế (nếu có access)\n",
        "2. **Đánh giá trên dataset lớn hơn**\n",
        "3. **So sánh với baseline models** (standard T5, GPT, etc.)\n",
        "4. **Implement human evaluation** cho educational quality\n",
        "5. **Fine-tune** cho specific English learning tasks\n",
        "\n",
        "### Lưu ý quan trọng:\n",
        "- **Đánh giá chất lượng NLG** không chỉ dựa vào metrics\n",
        "- **Human evaluation** cần thiết cho educational content\n",
        "- **Multiple metrics** cung cấp cái nhìn toàn diện\n",
        "- **Domain expertise** quan trọng cho English learning applications\n",
        "\n",
        "**Happy Evaluating! 🚀📊**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
