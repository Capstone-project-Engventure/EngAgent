# ğŸ§  Memory Optimization & DeepSeek Integration Guide

## ğŸš¨ Giáº£i quyáº¿t lá»—i Memory vá»›i Ollama

### Lá»—i hiá»‡n táº¡i:
```
ValueError: Ollama call failed with status code 500. 
Details: {"error":"model requires more system memory (4.8 GiB) than is available (1.9 GiB)"}
```

### ğŸ’¡ CÃ¡c giáº£i phÃ¡p:

#### 1. **Sá»­ dá»¥ng model nhá» hÆ¡n (Khuyáº¿n nghá»‹)**

**TrÆ°á»›c khi báº¯t Ä‘áº§u, hÃ£y cháº¡y script setup:**
```bash
cd run_LLM
python setup_models.py
```

**Hoáº·c cÃ i Ä‘áº·t thá»§ cÃ´ng:**

```bash
# Model nhá» (~1.3GB) - phÃ¹ há»£p vá»›i RAM 2GB
ollama pull llama3.2:1b

# Model trung bÃ¬nh (~2.0GB) - phÃ¹ há»£p vá»›i RAM 3-4GB  
ollama pull llama3.2:3b

# Model lá»›n (~4.8GB) - cáº§n RAM 6GB+
ollama pull mistral
```

**Cáº­p nháº­t file `.env`:**
```env
# Chá»n model phÃ¹ há»£p vá»›i RAM cá»§a báº¡n
OLLAMA_MODEL=llama3.2:1b   # Cho RAM tháº¥p
# OLLAMA_MODEL=llama3.2:3b   # Cho RAM trung bÃ¬nh
# OLLAMA_MODEL=mistral       # Cho RAM cao
```

#### 2. **Tá»‘i Æ°u hÃ³a Ollama**

**Giáº£m context length:**
```bash
# Táº¡o Modelfile tÃ¹y chá»‰nh
cat > Modelfile <<EOF
FROM llama3.2:1b
PARAMETER num_ctx 2048
PARAMETER num_predict 512
EOF

# Táº¡o model tÃ¹y chá»‰nh
ollama create llama3.2-optimized -f Modelfile
```

**Cáº­p nháº­t `.env`:**
```env
OLLAMA_MODEL=llama3.2-optimized
OLLAMA_TIMEOUT=300
```

#### 3. **Sá»­ dá»¥ng DeepSeek API (Khuyáº¿n nghá»‹ cao)**

DeepSeek cung cáº¥p API máº¡nh máº½, khÃ´ng cáº§n RAM local:

**ÄÄƒng kÃ½ DeepSeek API:**
1. Truy cáº­p: https://platform.deepseek.com
2. ÄÄƒng kÃ½ tÃ i khoáº£n
3. Láº¥y API key tá»« dashboard

**Cáº¥u hÃ¬nh DeepSeek:**
```env
USE_DEEPSEEK=true
DEEPSEEK_API_KEY=your_api_key_here
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_BASE_URL=https://api.deepseek.com
```

## ğŸŒŸ CÃ¡ch sá»­ dá»¥ng DeepSeek

### 1. **API Call vá»›i DeepSeek**

```bash
# Test API vá»›i DeepSeek
curl -X POST "http://localhost:8000/api/exercises/no-rag?modelType=deepseek" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt_name": "english_exercise_default",
    "number": 5,
    "type": "mcq",
    "skill": "grammar",
    "level": "intermediate",
    "topic": "present perfect"
  }'
```

### 2. **Fallback Strategy**

Há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng fallback khi má»™t model gáº·p lá»—i:

```
DeepSeek (Æ°u tiÃªn) â†’ Vertex AI â†’ Ollama (local)
```

### 3. **Kiá»ƒm tra tráº¡ng thÃ¡i**

```bash
curl http://localhost:8000/health
```

Káº¿t quáº£ sáº½ hiá»ƒn thá»‹ tráº¡ng thÃ¡i cá»§a táº¥t cáº£ models:
```json
{
  "status": "healthy",
  "components": {
    "ollama_llm": {"ok": true},
    "vertex_llm": {"ok": false},
    "deepseek_llm": {"ok": true},
    "config": {
      "ollama_model": "llama3.2:1b",
      "use_deepseek": true,
      "deepseek_configured": true
    }
  }
}
```

## ğŸ”§ CÃ i Ä‘áº·t vÃ  cáº¥u hÃ¬nh

### 1. **CÃ i Ä‘áº·t dependencies**

```bash
cd run_LLM
pip install -r requirements.txt
```

### 2. **Cháº¡y setup script**

```bash
python setup_models.py
```

Script nÃ y sáº½:
- Kiá»ƒm tra RAM há»‡ thá»‘ng
- Äá» xuáº¥t model phÃ¹ há»£p
- CÃ i Ä‘áº·t Ollama model
- Cáº¥u hÃ¬nh DeepSeek (optional)
- Táº¡o file `.env`

### 3. **Test há»‡ thá»‘ng**

```bash
# Khá»Ÿi Ä‘á»™ng service
python -m uvicorn app.main:app --reload

# Test health check
curl http://localhost:8000/health

# Test API vá»›i model nhá»
curl -X POST "http://localhost:8000/api/exercises/no-rag?modelType=ollama" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt_name": "english_exercise_default",
    "number": 2,
    "type": "mcq", 
    "skill": "vocabulary",
    "level": "beginner",
    "topic": "animals"
  }'
```

## ğŸ“Š So sÃ¡nh Models

| Model | RAM Required | Performance | Cost | Use Case |
|-------|-------------|-------------|------|----------|
| `llama3.2:1b` | ~1.3GB | Good | Free | Development, testing |
| `llama3.2:3b` | ~2.0GB | Better | Free | Small production |
| `mistral` | ~4.8GB | Excellent | Free | High-end local |
| `DeepSeek API` | Minimal | Excellent | Paid | Production recommended |

## ğŸš€ Khuyáº¿n nghá»‹ Production

### Setup lai tá»± Ä‘á»™ng:
```bash
# 1. Setup mÃ´i trÆ°á»ng
python setup_models.py

# 2. Cáº¥u hÃ¬nh DeepSeek cho production  
echo "USE_DEEPSEEK=true" >> .env
echo "DEEPSEEK_API_KEY=your_key" >> .env

# 3. Backup vá»›i Ollama model nhá»
echo "OLLAMA_MODEL=llama3.2:1b" >> .env

# 4. Test táº¥t cáº£ models
python -c "
import requests
models = ['deepseek', 'ollama']
for model in models:
    try:
        resp = requests.get(f'http://localhost:8000/health')
        print(f'{model}: OK' if resp.status_code == 200 else f'{model}: Error')
    except:
        print(f'{model}: Offline')
"
```

## ğŸ”§ Troubleshooting

### Lá»—i Memory váº«n cÃ²n:
1. Restart Ollama: `ollama serve`
2. Clear model cache: `ollama rm <model_name>`
3. Sá»­ dá»¥ng model nhá» hÆ¡n
4. Chuyá»ƒn sang DeepSeek API

### DeepSeek API khÃ´ng hoáº¡t Ä‘á»™ng:
1. Kiá»ƒm tra API key
2. Verify internet connection  
3. Check API quota/billing

### Performance tháº¥p:
1. TÄƒng `OLLAMA_TIMEOUT` trong `.env`
2. Sá»­ dá»¥ng GPU náº¿u cÃ³
3. Close cÃ¡c á»©ng dá»¥ng khÃ¡c

## ğŸ“ Support

Náº¿u váº«n gáº·p váº¥n Ä‘á»:
1. Cháº¡y `python setup_models.py` Ä‘á»ƒ re-configure
2. Check logs: `tail -f /var/log/ollama.log`
3. Test tá»«ng component riÃªng láº»
4. Sá»­ dá»¥ng DeepSeek API lÃ m primary solution 